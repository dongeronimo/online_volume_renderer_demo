#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DICOM to Float16 Raw Buffer Converter with GPU Perona-Malik Smoothing
Converts DICOM series to raw float16 buffers for WebGPU volume rendering.
Includes optional anonymization for patient data protection.
"""

import os
import sys
import json
import argparse
import hashlib
from pathlib import Path
from typing import List, Dict, Any
import numpy as np
import pydicom
from pydicom.errors import InvalidDicomError

# Optional GPU acceleration
try:
    import cupy as cp
    HAS_CUPY = True
    print("✓ CuPy detected - GPU acceleration available")
except ImportError:
    HAS_CUPY = False
    print("⚠  CuPy not available - smoothing will be skipped or use CPU (slow)")


def anonymize_identifier(identifier: str, salt: str = "medical_volume_renderer") -> str:
    """
    Anonymize an identifier using SHA256 hashing.
    
    Args:
        identifier: Original identifier to anonymize
        salt: Salt for the hash (default: project name)
        
    Returns:
        Anonymized hash (first 16 characters)
    """
    if not identifier:
        return "UNKNOWN"
    
    # Create hash
    hash_input = f"{salt}:{identifier}".encode('utf-8')
    hash_result = hashlib.sha256(hash_input).hexdigest()
    
    # Return first 16 characters for readability
    return hash_result[:16].upper()


def read_dicom_files(input_dir: Path) -> List[pydicom.FileDataset]:
    """
    Read all DICOM files from the input directory.
    
    Args:
        input_dir: Path to directory containing DICOM files
        
    Returns:
        List of DICOM datasets
    """
    dicom_files = []
    
    print(f"Scanning directory: {input_dir}")
    
    for file_path in input_dir.rglob("*"):
        if file_path.is_file():
            try:
                ds = pydicom.dcmread(str(file_path), force=True)
                # Verify it has pixel data
                if hasattr(ds, 'pixel_array'):
                    dicom_files.append(ds)
                    print(f"  ✓ Loaded: {file_path.name}")
                else:
                    print(f"  ⚠  Skipped (no pixel data): {file_path.name}")
            except InvalidDicomError:
                print(f"  ✗ Skipped (invalid DICOM): {file_path.name}")
            except Exception as e:
                print(f"  ✗ Error reading {file_path.name}: {e}")
    
    print(f"\nFound {len(dicom_files)} valid DICOM files")
    return dicom_files


def sort_dicom_slices(dicom_files: List[pydicom.FileDataset]) -> List[pydicom.FileDataset]:
    """
    Sort DICOM slices by Instance Number or Slice Location.
    
    Args:
        dicom_files: List of unsorted DICOM datasets
        
    Returns:
        Sorted list of DICOM datasets
    """
    def get_sort_key(ds):
        # Try different sorting methods in order of preference
        if hasattr(ds, 'InstanceNumber') and ds.InstanceNumber is not None:
            return float(ds.InstanceNumber)
        elif hasattr(ds, 'SliceLocation') and ds.SliceLocation is not None:
            return float(ds.SliceLocation)
        elif hasattr(ds, 'ImagePositionPatient') and ds.ImagePositionPatient is not None:
            # Use Z coordinate (third value)
            return float(ds.ImagePositionPatient[2])
        else:
            return 0
    
    sorted_files = sorted(dicom_files, key=get_sort_key)
    print(f"Sorted {len(sorted_files)} slices")
    return sorted_files


def extract_metadata(dicom_files: List[pydicom.FileDataset], global_min: float, global_max: float, anonymize: bool = True) -> Dict[str, Any]:
    """
    Extract relevant metadata from DICOM series.
    
    Args:
        dicom_files: List of DICOM datasets
        global_min: Minimum HU value across all slices
        global_max: Maximum HU value across all slices
        anonymize: Whether to anonymize patient identifiers
        
    Returns:
        Dictionary containing series metadata
    """
    if not dicom_files:
        return {}
    
    # Use first file for series-level metadata
    first_ds = dicom_files[0]
    
    # Helper function to safely get DICOM tag value
    def safe_get(ds, attr, default=""):
        try:
            val = getattr(ds, attr, default)
            # Convert to string and handle potential encoding issues
            if isinstance(val, bytes):
                return val.decode('utf-8', errors='replace')
            elif isinstance(val, pydicom.multival.MultiValue):
                return [str(v) for v in val]
            return str(val) if val != default else default
        except:
            return default
    
    # Get pixel array shape from first slice
    pixel_array = first_ds.pixel_array
    
    # Get original identifiers
    patient_name = safe_get(first_ds, 'PatientName')
    patient_id = safe_get(first_ds, 'PatientID')
    study_uid = safe_get(first_ds, 'StudyInstanceUID')
    series_uid = safe_get(first_ds, 'SeriesInstanceUID')
    
    # Anonymize if requested
    if anonymize:
        patient_name = f"ANON_{anonymize_identifier(patient_name)}"
        patient_id = f"ID_{anonymize_identifier(patient_id)}"
        # Keep UID structure but anonymize - ensure we don't exceed bounds
        study_uid_str = str(study_uid)
        series_uid_str = str(series_uid)
        study_uid = f"STUDY_{anonymize_identifier(study_uid_str[:min(len(study_uid_str), 64)])}"
        series_uid = f"SERIES_{anonymize_identifier(series_uid_str[:min(len(series_uid_str), 64)])}"
    
    metadata = {
        "numSlices": len(dicom_files),
        "width": int(pixel_array.shape[1]),
        "height": int(pixel_array.shape[0]),
        "format": "float16",
        "bytesPerVoxel": 2,
        "anonymized": anonymize,
        
        # Patient information (anonymized if requested)
        "patientName": patient_name,
        "patientID": patient_id,
        "patientBirthDate": safe_get(first_ds, 'PatientBirthDate'),
        "patientSex": safe_get(first_ds, 'PatientSex'),
        
        # Study information
        "studyDate": safe_get(first_ds, 'StudyDate'),
        "studyTime": safe_get(first_ds, 'StudyTime'),
        "studyDescription": safe_get(first_ds, 'StudyDescription'),
        "studyInstanceUID": study_uid,
        
        # Series information
        "seriesNumber": safe_get(first_ds, 'SeriesNumber'),
        "seriesDescription": safe_get(first_ds, 'SeriesDescription'),
        "seriesInstanceUID": series_uid,
        "modality": safe_get(first_ds, 'Modality'),
        
        # Image information
        "pixelSpacing": safe_get(first_ds, 'PixelSpacing'),
        "sliceThickness": safe_get(first_ds, 'SliceThickness'),
        "imageOrientationPatient": safe_get(first_ds, 'ImageOrientationPatient'),
        "imagePositionPatient": safe_get(first_ds, 'ImagePositionPatient'),
        
        # Window/Level (for display)
        "windowCenter": safe_get(first_ds, 'WindowCenter'),
        "windowWidth": safe_get(first_ds, 'WindowWidth'),
        
        # Rescale parameters
        "rescaleSlope": safe_get(first_ds, 'RescaleSlope', '1.0'),
        "rescaleIntercept": safe_get(first_ds, 'RescaleIntercept', '0.0'),
        
        # Value range (in Hounsfield Units for CT)
        "huMin": float(global_min),
        "huMax": float(global_max),
    }
    
    return metadata


def perona_malik_gpu(volume: np.ndarray, iterations: int = 5, K: float = 50.0, 
                     lambda_param: float = 0.1, diffusion_type: int = 1) -> np.ndarray:
    """
    Apply Perona-Malik anisotropic diffusion using GPU acceleration via CuPy.
    
    Args:
        volume: 3D numpy array in HU values (not normalized)
        iterations: Number of diffusion iterations
        K: Edge threshold parameter (in HU units, default 50.0 for CT data)
        lambda_param: Time step (stability requires lambda <= 0.25 for 3D)
        diffusion_type: 1 (exponential) or 2 (rational)
        
    Returns:
        Smoothed volume as numpy array in HU values
    """
    if not HAS_CUPY:
        print("⚠  CuPy not available - returning unsmoothed volume")
        return volume
    
    print(f"Applying Perona-Malik smoothing on GPU ({iterations} iterations)...")
    
    # Transfer to GPU
    vol_gpu = cp.asarray(volume, dtype=cp.float32)
    output_gpu = cp.zeros_like(vol_gpu)
    
    for iteration in range(iterations):
        # Compute gradients using neighbor differences
        # Pad for boundary handling
        padded = cp.pad(vol_gpu, 1, mode='edge')
        
        # Extract neighbors (6-connected)
        north = padded[:-2, 1:-1, 1:-1]
        south = padded[2:, 1:-1, 1:-1]
        west = padded[1:-1, :-2, 1:-1]
        east = padded[1:-1, 2:, 1:-1]
        up = padded[1:-1, 1:-1, :-2]
        down = padded[1:-1, 1:-1, 2:]
        center = vol_gpu
        
        # Compute gradient magnitudes
        grad_n = cp.abs(north - center)
        grad_s = cp.abs(south - center)
        grad_w = cp.abs(west - center)
        grad_e = cp.abs(east - center)
        grad_u = cp.abs(up - center)
        grad_d = cp.abs(down - center)
        
        # Compute diffusion coefficients
        if diffusion_type == 1:
            # Exponential: favors high-contrast edges
            c_n = cp.exp(-(grad_n / K) ** 2)
            c_s = cp.exp(-(grad_s / K) ** 2)
            c_w = cp.exp(-(grad_w / K) ** 2)
            c_e = cp.exp(-(grad_e / K) ** 2)
            c_u = cp.exp(-(grad_u / K) ** 2)
            c_d = cp.exp(-(grad_d / K) ** 2)
        else:
            # Rational: favors wide regions
            c_n = 1.0 / (1.0 + (grad_n / K) ** 2)
            c_s = 1.0 / (1.0 + (grad_s / K) ** 2)
            c_w = 1.0 / (1.0 + (grad_w / K) ** 2)
            c_e = 1.0 / (1.0 + (grad_e / K) ** 2)
            c_u = 1.0 / (1.0 + (grad_u / K) ** 2)
            c_d = 1.0 / (1.0 + (grad_d / K) ** 2)
        
        # Compute divergence of diffusion flux
        divergence = (
            c_n * (north - center) +
            c_s * (south - center) +
            c_w * (west - center) +
            c_e * (east - center) +
            c_u * (up - center) +
            c_d * (down - center)
        )
        
        # Update: I(t+1) = I(t) + lambda * divergence
        output_gpu[:] = center + lambda_param * divergence
        
        # Swap buffers for next iteration
        vol_gpu, output_gpu = output_gpu, vol_gpu
        
        if (iteration + 1) % 1 == 0:
            print(f"  Iteration {iteration + 1}/{iterations} complete")
    
    # Transfer back to CPU
    result = cp.asnumpy(vol_gpu)
    
    print("✓ Smoothing complete")
    return result


def compute_chunk_minmax(volume: np.ndarray, chunk_size: int) -> np.ndarray:
    """
    Compute min/max values for each chunk in the volume.
    
    Args:
        volume: 3D numpy array (Z, Y, X) with values in [0, 1]
        chunk_size: Size of each cubic chunk (must be power of 2)
        
    Returns:
        Numpy array of shape (num_chunks_z, num_chunks_y, num_chunks_x, 2)
        where [:, :, :, 0] is min and [:, :, :, 1] is max
    """
    depth, height, width = volume.shape
    
    # Round up to next multiple of chunk_size
    padded_depth = ((depth + chunk_size - 1) // chunk_size) * chunk_size
    padded_height = ((height + chunk_size - 1) // chunk_size) * chunk_size
    padded_width = ((width + chunk_size - 1) // chunk_size) * chunk_size
    
    # Pad volume if necessary
    if padded_depth > depth or padded_height > height or padded_width > width:
        padded_volume = np.zeros((padded_depth, padded_height, padded_width), dtype=volume.dtype)
        padded_volume[:depth, :height, :width] = volume
        volume = padded_volume
    
    num_chunks_z = padded_depth // chunk_size
    num_chunks_y = padded_height // chunk_size
    num_chunks_x = padded_width // chunk_size
    
    print(f"\nComputing chunk min/max values...")
    print(f"  Chunk size: {chunk_size}³")
    print(f"  Volume size: {width}×{height}×{depth}")
    print(f"  Padded size: {padded_width}×{padded_height}×{padded_depth}")
    print(f"  Number of chunks: {num_chunks_x}×{num_chunks_y}×{num_chunks_z} = {num_chunks_x * num_chunks_y * num_chunks_z}")
    
    # Allocate result array
    chunk_minmax = np.zeros((num_chunks_z, num_chunks_y, num_chunks_x, 2), dtype=np.float32)
    
    total_chunks = num_chunks_x * num_chunks_y * num_chunks_z
    processed = 0
    
    # Compute min/max for each chunk
    for iz in range(num_chunks_z):
        for iy in range(num_chunks_y):
            for ix in range(num_chunks_x):
                z_start = iz * chunk_size
                y_start = iy * chunk_size
                x_start = ix * chunk_size
                
                z_end = z_start + chunk_size
                y_end = y_start + chunk_size
                x_end = x_start + chunk_size
                
                chunk = volume[z_start:z_end, y_start:y_end, x_start:x_end]
                
                chunk_minmax[iz, iy, ix, 0] = np.min(chunk)
                chunk_minmax[iz, iy, ix, 1] = np.max(chunk)
                
                processed += 1
                if processed % 100 == 0 or processed == total_chunks:
                    print(f"  Processed {processed}/{total_chunks} chunks", end='\r')
    
    print(f"\n✓ Chunk min/max computation complete")
    
    return chunk_minmax


def convert_dicom_series(input_dir: Path, output_dir: Path, apply_smoothing: bool = True,
                        smoothing_iterations: int = 5, chunk_size: int = 32, anonymize: bool = True):
    """
    Convert DICOM series to float16 raw buffers and metadata JSON.
    
    Args:
        input_dir: Directory containing DICOM files
        output_dir: Directory to save output files
        apply_smoothing: Whether to apply Perona-Malik smoothing
        smoothing_iterations: Number of smoothing iterations
        chunk_size: Size of cubic chunks for min/max computation (must be multiple of 16)
        anonymize: Whether to anonymize patient identifiers
    """
    # Validate chunk size
    if chunk_size % 16 != 0:
        raise ValueError(f"Chunk size must be a multiple of 16, got {chunk_size}")
    if chunk_size < 16 or chunk_size > 256:
        raise ValueError(f"Chunk size must be between 16 and 256, got {chunk_size}")
    
    # Create output directory
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Read and sort DICOM files
    dicom_files = read_dicom_files(input_dir)
    if not dicom_files:
        print("✗ No valid DICOM files found!")
        return
    
    sorted_files = sort_dicom_slices(dicom_files)
    
    # First pass - find GLOBAL min/max and load volume
    print("\nLoading volume into memory...")
    first_ds = sorted_files[0]
    height, width = first_ds.pixel_array.shape
    num_slices = len(sorted_files)
    
    # Allocate full volume
    volume = np.zeros((num_slices, height, width), dtype=np.float32)
    global_min = float('inf')
    global_max = float('-inf')
    
    for i, ds in enumerate(sorted_files):
        rescale_slope = float(getattr(ds, 'RescaleSlope', 1.0))
        rescale_intercept = float(getattr(ds, 'RescaleIntercept', 0.0))
        pixel_array = ds.pixel_array
        data = pixel_array.astype(np.float32) * rescale_slope + rescale_intercept
        
        slice_min = np.min(data)
        slice_max = np.max(data)
        
        global_min = min(global_min, slice_min)
        global_max = max(global_max, slice_max)
        
        volume[i] = data
        
        if (i + 1) % 50 == 0 or i == num_slices - 1:
            print(f"  Loaded {i + 1}/{num_slices} slices")
    
    print(f"  Global HU range: [{global_min:.1f}, {global_max:.1f}]")
    
    # Keep HU values as-is (no normalization)
    print("\nKeeping original HU values (no normalization)...")
    
    # Apply smoothing if requested
    if apply_smoothing and HAS_CUPY:
        volume = perona_malik_gpu(volume, iterations=smoothing_iterations, 
                                  K=50.0, lambda_param=0.1, diffusion_type=2)
    elif apply_smoothing and not HAS_CUPY:
        print("⚠  Smoothing requested but CuPy not available - saving unsmoothed volume")
    
    # Compute chunk min/max (after smoothing so it reflects actual rendered values)
    chunk_minmax = compute_chunk_minmax(volume, chunk_size)
    
    # Convert to float16 and save slices
    print(f"\nSaving {num_slices} slices as float16...")
    volume_f16 = volume.astype(np.float16)
    
    for i in range(num_slices):
        output_file = output_dir / f"slice_{i:04d}.raw"
        volume_f16[i].tofile(str(output_file))
        
        if (i + 1) % 50 == 0 or i == num_slices - 1:
            print(f"  Saved {i + 1}/{num_slices} slices")
    
    # Save chunk min/max data as binary file
    chunk_file = output_dir / "chunk_minmax.bin"
    print(f"\nSaving chunk min/max data to: {chunk_file}")
    chunk_minmax.astype(np.float32).tofile(str(chunk_file))
    print(f"  Chunk data size: {chunk_minmax.nbytes / (1024*1024):.2f} MB")
    
    # Extract and save metadata
    print("\nExtracting metadata...")
    metadata = extract_metadata(sorted_files, global_min, global_max, anonymize)
    
    # Add chunk information to metadata
    metadata["chunkSize"] = chunk_size
    metadata["numChunksX"] = int(chunk_minmax.shape[2])
    metadata["numChunksY"] = int(chunk_minmax.shape[1])
    metadata["numChunksZ"] = int(chunk_minmax.shape[0])
    metadata["totalChunks"] = int(chunk_minmax.shape[0] * chunk_minmax.shape[1] * chunk_minmax.shape[2])
    
    metadata_file = output_dir / "metadata.json"
    print(f"Saving metadata to: {metadata_file}")
    
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print("\n✅ Conversion complete!")
    print(f"   Output directory: {output_dir}")
    print(f"   Slice files: slice_0000.raw to slice_{num_slices-1:04d}.raw")
    print(f"   Chunk data: chunk_minmax.bin")
    print(f"   Metadata: metadata.json")
    print(f"   Volume dimensions: {width}x{height}x{num_slices}")
    print(f"   Chunk configuration: {chunk_size}³ chunks")
    print(f"   Total chunks: {metadata['numChunksX']}×{metadata['numChunksY']}×{metadata['numChunksZ']} = {metadata['totalChunks']}")
    print(f"   HU range: [{metadata['huMin']:.1f}, {metadata['huMax']:.1f}]")
    print(f"   Anonymization: {'Enabled' if anonymize else 'Disabled'}")
    if apply_smoothing and HAS_CUPY:
        print(f"   Smoothing: Applied ({smoothing_iterations} iterations)")
    else:
        print(f"   Smoothing: Skipped")
    

def main():
    parser = argparse.ArgumentParser(
        description='Convert DICOM series to float16 raw buffers for WebGPU',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python dicom_converter.py -i ./dicom_data -o ./output
  python dicom_converter.py -i ./dicom_data -o ./output --no-smooth
  python dicom_converter.py -i ./dicom_data -o ./output --iterations 10
  python dicom_converter.py -i ./dicom_data -o ./output --no-anonymize
  python dicom_converter.py -i ./dicom_data -o ./output --chunk-size 64 --no-smooth
        """
    )
    
    parser.add_argument(
        '-i', '--input',
        type=str,
        required=True,
        help='Input directory containing DICOM files'
    )
    
    parser.add_argument(
        '-o', '--output',
        type=str,
        required=True,
        help='Output directory for raw buffers and metadata'
    )
    
    parser.add_argument(
        '--no-smooth',
        action='store_true',
        help='Skip Perona-Malik smoothing (faster, less memory)'
    )
    
    parser.add_argument(
        '--iterations',
        type=int,
        default=5,
        help='Number of smoothing iterations (default: 5)'
    )
    
    parser.add_argument(
        '--chunk-size',
        type=int,
        default=32,
        help='Size of cubic chunks for min/max computation (must be multiple of 16, default: 32)'
    )
    
    parser.add_argument(
        '--no-anonymize',
        action='store_true',
        help='Do not anonymize patient identifiers (keep original names/IDs)'
    )
    
    args = parser.parse_args()
    
    input_dir = Path(args.input)
    output_dir = Path(args.output)
    
    # Validate chunk size
    if args.chunk_size % 16 != 0:
        print(f"✗ Error: Chunk size must be a multiple of 16, got {args.chunk_size}")
        sys.exit(1)
    
    if args.chunk_size < 16 or args.chunk_size > 256:
        print(f"✗ Error: Chunk size must be between 16 and 256, got {args.chunk_size}")
        sys.exit(1)
    
    # Validate input directory
    if not input_dir.exists():
        print(f"✗ Error: Input directory does not exist: {input_dir}")
        sys.exit(1)
    
    if not input_dir.is_dir():
        print(f"✗ Error: Input path is not a directory: {input_dir}")
        sys.exit(1)
    
    # Warn if not anonymizing
    anonymize = not args.no_anonymize
    if not anonymize:
        print("\n" + "="*60)
        print("⚠  WARNING: Patient data will NOT be anonymized!")
        print("="*60)
        response = input("Continue without anonymization? (yes/no): ")
        if response.lower() not in ['yes', 'y']:
            print("Conversion cancelled.")
            sys.exit(0)
        print()
    
    print("="*60)
    print("DICOM to Float16 Converter with GPU Smoothing")
    print("="*60)
    print(f"Input:  {input_dir}")
    print(f"Output: {output_dir}")
    print(f"Chunk size: {args.chunk_size}³")
    print(f"Smoothing: {'Disabled' if args.no_smooth else f'Enabled ({args.iterations} iterations)'}")
    print(f"Anonymization: {'Enabled' if anonymize else 'Disabled'}")
    print("="*60 + "\n")
    
    try:
        convert_dicom_series(input_dir, output_dir, 
                           apply_smoothing=not args.no_smooth,
                           smoothing_iterations=args.iterations,
                           chunk_size=args.chunk_size,
                           anonymize=anonymize)
    except Exception as e:
        print(f"\n✗ Error during conversion: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()